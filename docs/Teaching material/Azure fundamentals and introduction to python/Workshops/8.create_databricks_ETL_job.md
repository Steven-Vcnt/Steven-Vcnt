# Create an ETL job on Databricks

First, we will create a new notebook that will store the code for our ETL.

## Setup the ETL notebook

```sql title="Query the ticker to fetch"
sp_ticker_list = spark.sql('''
            SELECT DISTINCT 
              * 
            FROM 
              default.ticker_listing tl
            LEFT JOIN bronze.ticker_metadata tm
                ON tl.symbol=tm.Ticker
            WHERE 
              tm.Ticker IS NULL
            LIMIT 10
              ''')
```

```python title="Import Package"
import requests
import pandas as pd
import json
import os
from datetime import datetime
pd.set_option('display.max_columns', 500)
AV_API_Key = 'YourAPIKey'
```

```python title="Fetch the ticker with the AlphaVanta API"
# Convert PySpark DataFrame to a Pandas DataFrame and store the value in a list
ticker_list = sp_ticker_list.toPandas().values.tolist()
# Initialize empty DataFrame
ticker_time_series=pd.DataFrame()
ticker_metadata=pd.DataFrame()
# Loop on the ticker list
for ticker in ticker_list :
    r_stock = requests.get('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&outputsize=full&symbol={ticker}&apikey={apiKey}'.format(apiKey=AV_API_Key, ticker=ticker))
    js_stock = r_stock.json()
    try:
        # Ticker time series to DataFrame
        df_time_series=pd.DataFrame(js_stock['Time Series (Daily)'])
        df_time_series['Ticker'] = ticker[0]
        ticker_time_series = pd.concat([ticker_time_series, df_time_series])
        # Ticker metadata to DataFrame
        df_metadata=pd.json_normalize(js_stock['Meta Data'])
        df_metadata['Ticker'] = ticker[0]
        df_metadata['Status'] = 'active'
        df_metadata['Date'] = datetime.now()
    except:
        df_metadata = pd.DataFrame({"Ticker" : [ticker[0]], "Status": ["inactive"] , "Date": [datetime.now()], 'Information':[None], 'Symbol': [None], 'Last_Refreshed' : [None], 'Output_Size' : [None], 'Time_Zone' : [None]})
    ticker_metadata = pd.concat([ticker_metadata, df_metadata])
display(ticker_metadata.head())
```

```python title="Upsert ticker value data "
try:
  ticker_time_series=ticker_time_series.reset_index().rename({'index':'Value_type'}, axis=1)
  lg_ticker_ts = ticker_time_series.melt(id_vars=['Ticker','Value_type'])
  display(lg_ticker_ts.head())
  sp_ticker_ts=spark.createDataFrame(lg_ticker_ts)
  sp_ticker_ts.distinct().createOrReplaceTempView('sp_ticker_temp')
  spark.sql(''' 
            MERGE INTO 
              bronze.ticker_value 
            USING  sp_ticker_temp 
              ON bronze.ticker_value.Ticker = sp_ticker_temp.Ticker
            WHEN MATCHED THEN
              UPDATE SET  *
            WHEN NOT MATCHED THEN
              INSERT  *
            ''')
except:
  pass

```

```python title="Create temp view"
sp_ticker_metadata=spark.createDataFrame(ticker_metadata)
sp_ticker_metadata.distinct().createOrReplaceTempView('ticker_metadata_temp')
```

```sql title="Upsert Ticker metadata delta table"
%sql MERGE INTO bronze.ticker_metadata USING ticker_metadata_temp ON bronze.ticker_metadata.Ticker = ticker_metadata_temp.Ticker
WHEN MATCHED THEN
UPDATE
SET
  *
  WHEN NOT MATCHED THEN
INSERT
  *
```

## Set up the schedule with Databricks Jobs

We will now schedule a daily job to extract ticker 10 by 10 every day.
Go to Databricks **Workflows>Create Job**

![image](../../../assets/images/azure_databricks_create_job.png)

Add the following parameters to your task and then create a task:

- Task name: extract_ticker_value
- Type: Notebook
- Source: Workspace
- Path: Your notebook path
- Cluster: Your active cluster

![image](../../../assets/images/azure_databricks_create_task.png)

Lastly, modify your job to add a scheduler:

![image](../../../assets/images/azure_databricks_modify_job.png)

Add the following schedule:

- Trigger Status: Active
- Trigger type: Scheduled
- Schedule: Every Day at 3 AM UTC +01:00

![image](../../../assets/images/azure_databricks_job_schedule.png)

Well done, you can run your job with the button **Run now** to verify if everything works!

You should see your run as succeeded in the **Job Runs** section:

![image](../../../assets/images/azure_databricks_job_run.png)

---
